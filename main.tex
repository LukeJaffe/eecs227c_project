\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[sorting=none]{biblatex}
\addbibresource{references.bib}

\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{Luke Jaffe}
\title{
    EECS227C Project \\
    \large Convergence Analysis for Gradient Methods on Metric Learning Objectives
    }
\maketitle

\medskip

\section{Introduction}

In this work we study:
\begin{itemize}
    \item Two metric learning objectives
    \item Relaxation and reformulation of these objectives as DC functions
    \item Smoothability of these objectives, and study of accuracy/smoothness tradeoff
    \item Dependence on data of conditioning of the problem
    \item Study of smoothness parameters vs. minima for original and smooth relaxation
    \item Interpretations of these objectives as learning a separating hyperplane with soft margin
    \item Extension of the objectives to learning an affine mapping instead of weight vector
    \item Different gradient methods for minimization of these objectives
    \item Possible extensions to the empirical risk minimization setting
\end{itemize}


Other stuff:
\begin{itemize}
    \item Different learning rate selection including exact line search, backtracking line search
    \item Nesterov momentum for speedup (for badly conditioned problems?)
\end{itemize}

\section{Initial writeup for simplified problem}

\subsection{DC Reformulation}

\subsection{Analysis of Full Function Minima}

 

\subsection{Smoothness Analysis of $g(w)$}

\begin{equation}
    g(w) = \log( \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) )
\end{equation}

\begin{equation}
    \nabla g(w) 
    = \frac{ 2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} w}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{top}) 
    &= 4 \{ \exp((w^{\top}u)^2)uu^{\top}ww^{\top}uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}ww^{\top}vv^{\top} \} \\
    &= 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \}
\end{align*}

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{bot}) = \frac{ -2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top} \} w }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 g(w) &= 
    \frac{ 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \} }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \\
    &- \frac{ 4 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} ww^{\top} \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Re-arranging, we have:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \left\{ \frac{  \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } +
    \frac{  \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\} \\
    &- 4 \left\{
    \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\}
    ww^{\top} \\
     & \left\{ \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  }
     \right\}
\end{align*}

We can simplify further by using the substitution: 
\begin{align*}
p = \frac{\exp((w^{\top}u)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Note that $0 < p < 1$, and:
\begin{align*}
1 - p = \frac{\exp((w^{\top}v)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Plugging in to the Hessian expression:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \{ p(w^{\top}u)^2 uu^{\top} + (1-p)(w^{\top}v)^2 vv^{\top} \} \\
    &- 4 \{ p uu^{\top} + (1-p) vv^{\top} \}ww^{\top}\{ p uu^{\top} + (1-p) vv^{\top} \} \\
    &= 4 p(w^{\top}u)^2 uu^{\top} + 4 (1-p)(w^{\top}v)^2 vv^{\top} \\
    &- 4 p^2 (w^{\top}u)^2 uu^{\top} - 4 (1-p)^2 (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \\
    &= 4 p (1 - p) (w^{\top}u)^2 uu^{\top} + 4 p (1-p) (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top})
\end{align*}

Using $0 < p(1-p) \leq \frac{1}{4}$, we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
    z^{\top} \nabla^2 g(w) z &\leq
    z^{\top} \left\{ (w^{\top}u)^2 uu^{\top} + (w^{\top}v)^2 vv^{\top} - 2 (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \right\} z \\
    &\leq \|z\|_2^2 \left\{ (w^{\top}u)^2 \|u\|_2^2 + (w^{\top}v)^2 \|v\|_2^2 + 4 \left| (w^{\top}u)(w^{\top}v)(u^{\top}v) \right| \right\} \\
    &\leq \|z\|_2^2 \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 g(w) \preceq \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\} I_d
\end{equation}

Meaning that $g(w)$ is $M_g$-smooth, with $M_g = \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Smoothness Analysis of Full Function}

In this section, we analyze a simplified version of the target function, with $\mu = 1$ and $\alpha = 0$. We simplify notation by letting $u = x - y$ and $v = x - z$.

\begin{equation}
    f(w) = \log(\exp((w^{\top}u)^2 - (w^{\top}v)^2) + 1)
\end{equation}

We can simplify further by letting $B = uu^\top - vv^\top$:

\begin{equation}
    f(w) = \log(\exp(w^{\top}Bw) + 1)
\end{equation}

\begin{equation}
    \nabla f(w) = \frac{2 \exp(w^{\top}Bw)Bw}{\exp(w^{\top}Bw) + 1}
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{top}) = 4 \exp(w^{\top}Bw)Bww^{\top}B  + 2 \exp(w^{\top}Bw)B
\end{align*}

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{bot}) = \frac{-2 \exp(w^{\top}Bw)B}{\exp(w^{\top}Bw) + 1}
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 f(w) = \frac{4 \exp(w^{\top}Bw)B(ww^{\top}B + \frac{1}{2}I_d)}{\exp(w^{\top}Bw) + 1} - \frac{4 \{\exp(w^{\top}Bw)\}^2 Bww^{\top}B }{\{\exp(w^{\top}Bw) + 1\}^2}
\end{align*}

We can simplify further by using the substitution: 
\begin{align*}
q = \frac{\exp(w^{\top}Bw)}{\exp(w^{\top}Bw) + 1}
\end{align*}

Using this substitution, we have:
\begin{equation}
    \nabla^2 f(w) = 4q(1-q)Bww^{\top}B + 2qB
\end{equation}

Noting that $0 < q < 1$ and $0 < q^2 \leq \frac{1}{4}$ we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
z^{\top} \nabla^2 f(w) z 
&= 4q(1-q) z^{\top} Bww^{\top}B z + 2q z^{\top} B z \\
&< z^{\top} Bww^{\top}B z + 2 z^{\top} B z \\
&\leq \|z\|_2^2 \|w\|_2^2 \|B\|_2^2 + 2 \|z\|_2^2 \|B\|_2 \\
&= \|z\|_2^2 ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) 
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 f(w) \prec ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) I_d
\end{equation}

Meaning that $f(w)$ is $M_f$-smooth, with $M_f = \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2$.


\subsection{Gradient Descent Analysis of Full Function}

\subsection{Extension to case with $\mu \neq 1$ and $\alpha > 0$}

\subsection{Extension to case with $m > 1$}

\section{Analysis of Original Problem}

% ========== Begin answering questions here 
The \textit{triplet margin loss} function is a common metric learning objective function, which has been demonstrated as empirically effective in the deep learning literature \cite{schroff_facenet_2015, hermans_defense_2017}. Let $X, Y, Z \in \mathbb{R}^{n \times m}$ be a collection of data, with samples $x_i, y_i, z_i \in \mathbb{R}^m$ being rows from each matrix respectively, such that samples from $X$ and $Y$ have the same label, and samples from $Z$ have a different label. Let $f: \mathbb{R}^m \rightarrow \mathbb{R}^d$ be an \textit{embedding function} which projects samples into an \textit{embedding space}, $\mathbb{R}^d$. Let $\alpha \in \mathbb{R}^+$ be the margin parameter. Then the triplet margin loss can be stated as follows:

\begin{equation}
    \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(x_i) - f(y_i)\|_2^2 - \| f(x_i) - f(z_i)\|_2^2 + \alpha)
\end{equation}

Let $f$ be parameterized by some parameter set $\theta \in \Theta$, and restrict $\| f(\theta)\|_2 = 1 \quad \forall \theta \in \Theta$. Then we can use our objective for empirical risk minimization by solving the following optimization problem:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 + \alpha) \\
    \textrm{s.t.} \quad &  \| f(\theta)\|_2 = 1 \quad \forall \theta \in \Theta
\end{aligned}
\end{equation}

While this objective has seen widespread usage for learning image and word embeddings, there has been no corresponding convergence analysis to date, even for a simple case. The goal of this work is to present a convergence analysis for an affine choice of $f$: $f(A, b; x) = Ax + b$, $A \in \mathbb{R}^{d \times m}, b \in \mathbb{R}^d$. To simplify analysis of gradient methods, we will concatenate $A$ and $b$ and append a $1$ to $x$: $\tilde{A} = [A, b]$, $\tilde{x}^\top = [x^\top, 1]$. Now we have $f(\tilde{A}; \tilde{x}) = \tilde{A} \tilde{x}$, $\tilde{A} \in \mathbb{R}^{d \times m+1}$.

Then we can simplify the optimization problem as follows:

\begin{flalign*}
& \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 & \\
& = \| f(\theta; x_i) \|_2^2 + \| f(\theta; y_i) \|_2^2 - 2\langle f(\theta; x_i), f(\theta; y_i) \rangle - \| f(\theta; x_i) \|_2^2 - \| f(\theta; z_i) \|_2^2 + 2\langle f(\theta; x_i), f(\theta; z_i) \rangle \\
& = \| f(\theta; y_i) \|_2^2 - \| f(\theta; z_i) \|_2^2 + 2\langle f(\theta; x_i), f(\theta; z_i) - f(\theta; y_i) \rangle \\
& = 2\langle f(\theta; x_i), f(\theta; z_i) - f(\theta; y_i) \rangle \quad\text{(using $\| f(\theta)\|_2 = 1$)} \\
\end{flalign*}

Now substituting affine choice of $f$:

\begin{flalign*}
& = 2\langle f(\tilde{A}; \tilde{x_i}), f(\tilde{A}; \tilde{z_i}) - f(\tilde{A}; \tilde{y_i}) \rangle & \\
& = 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i})
\end{flalign*}

And our optimization problem can be written as:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha) \\
    \textrm{s.t.} \quad &  \| f(\tilde{A})\|_2 = 1
\end{aligned}
\end{equation}

We note that this problem is convex, but not smooth.

\bigskip

***\textbf{Note}: Actually, I am not sure if the problem can be written that way, since the constraint $\| f(\theta)\|_2 = 1$ is obtained in practice by dividing $f$ by its norm: $\| g(\theta)\|_2 = \frac{f(\theta)}{\| f(\theta)\|_2} \rightarrow \| g(\theta)\|_2 = 1$. In this case, the problem would look like:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \max\left(0, 2\left\langle \frac{\tilde{A} \tilde{x_i}}{\| \tilde{A} \tilde{x_i} \|_2}, \frac{\tilde{A} \tilde{z_i}}{\| \tilde{A} \tilde{z_i} \|_2} - \frac{\tilde{A} \tilde{y_i}}{\| \tilde{A} \tilde{y_i} \|_2} \right\rangle + \alpha\right)
\end{aligned}
\end{equation}

I am not sure if this can be reformulated as a convex problem.

\section{Analysis of Relaxed Problem}

To alleviate issues with the previous formulation, including non-smoothness of the max function, and the norm constraint, we can relax the problem to be easier to analyze, while retaining its high-level properties. Specifically, we will swap the max function for the softplus function, and remove the norm constraint altogether.

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \log{(1 + \exp{( \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 + \alpha)})}
\end{aligned}
\end{equation}

With affine choice of f:

\begin{equation}
\label{eq:smooth_problem}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \log{(1 + \exp{(\tilde{y_i}^\top\tilde{A}^\top\tilde{A}\tilde{y_i} - \tilde{z_i}^\top\tilde{A}^\top\tilde{A}\tilde{z_i} + 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha)})}
\end{aligned}
\end{equation}

To check convexity of this function, we only need to study the inner portion:

\begin{equation}
\begin{aligned}
g(\tilde{A}) = \tilde{y_i}^\top\tilde{A}^\top\tilde{A}\tilde{y_i} - \tilde{z_i}^\top\tilde{A}^\top\tilde{A}\tilde{z_i} + 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha
\end{aligned}
\end{equation}

We can check if the Hessian of this function is PSD:

\begin{align*}
\nabla{g(\tilde{A})}
&= 2\tilde{A}\tilde{y_i}\tilde{y_i}^\top - 2\tilde{A}\tilde{z_i}\tilde{z_i}^\top + 2\tilde{A}(\tilde{x_i}(\tilde{z_i} - \tilde{y_i})^\top + (\tilde{z_i} - \tilde{y_i})\tilde{x_i}^\top) \\
&= 2\tilde{A}(\tilde{y_i}\tilde{y_i}^\top - \tilde{z_i}\tilde{z_i}^\top + \tilde{x_i}(\tilde{z_i} - \tilde{y_i})^\top + (\tilde{z_i} - \tilde{y_i})\tilde{x_i}^\top)
\end{align*}

\begin{equation}
\begin{aligned}
\nabla^2 g(\tilde{A})
&= \textrm{4d tensor}
\end{aligned}
\end{equation}

The Hessian is a 4d tensor, which complicates our analysis, so we will look at the case where $\tilde{A}$ is a vector as well. 

Let $f(w; x) = w^{\top}x$. Then we have:
\begin{equation}
\begin{aligned}
g(w) = (w^{\top}y)^2 - (w^{\top}z)^2 + 2(w^{\top}x)(w^{\top}(z - y))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})w
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla^2 g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})
\end{aligned}
\end{equation}

It is clear that we must impose some additional normalization of the data to ensure the Hessian is PSD, but it's not clear what that should be. One possibility is to use the eigenvalues of the data-dependent term as follows. Let $Q = \nabla^2 g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})$. 

\begin{equation}
\begin{aligned}
\nabla^2 \tilde{g}(w) = Q - diag(\lambda_{min}(Q))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla \tilde{g}(w) = (Q - diag(\lambda_{min}(Q)))w
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\tilde{g}(w) = \frac{1}{2}w^{\top}(Q - diag(\lambda_{min}(Q)))w
\end{aligned}
\end{equation}

However, it is clear that this is trivial because the original problem is unbounded below if Q has a negative eigenvalue, and the new problem has a minimum of 0. The new problem doesn't do anything for us.

If it is possible to ensure that the problem in (\ref{eq:smooth_problem}) is convex and smooth, we can analyze it using the standard techniques, and hopefully find a bound which is better than the generic bound.

\section{DC Reformulation with Smoothing}

Let us return to the original max formulation to show that it is DC if we use L2-norm instead of L2-norm-squared. We also remove the norm constraint.

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(\theta; x_i) - f(\theta; y_i)\|_2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2 + \alpha)
\end{aligned}
\end{equation}

Let $g_i(\theta) = \| f(\theta; x_i) - f(\theta; y_i)\|_2 + \alpha, \quad h_i(\theta) = f(\theta; x_i) - f(\theta; z_i)\|_2$. Note that $g_i(\theta)$ and $h_i(\theta)$ are convex if $f(\theta)$ is affine. We can rewrite the objective as:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, g_i(\theta) - h_i(\theta))
\end{aligned}
\end{equation}

Split up the max function:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(g_i(\theta), h_i(\theta)) - h_i(\theta)
\end{aligned}
\end{equation}

Finally, we can split up the sum to show the full DC formulation:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(g_i(\theta), h_i(\theta)) - \frac{1}{n} \sum_{i=1}^{n} h_i(\theta)
\end{aligned}
\end{equation}

Although this is DC for affine choice of $f(\theta)$, $h_i(\theta)$ is not smooth due to the max function and the L2-norm. To produce a smooth, convex approximation to the max function, we can use the LogSumExp function:

\begin{equation}
\begin{aligned}
\textrm{LSE}(x_1, ..., x_n) = \log\sum_{i=1}^{n} \exp(x_i)
\end{aligned}
\end{equation}

To produce a smooth approximation to the L2-norm, we can use the following:

\begin{equation}
\label{eq:smooth_2norm}
\begin{aligned}
\| x \|_\beta = \sqrt{\| x \|_2^2 + \beta^2} - \beta, \quad \beta > 0, x \in \mathbb{R}^d
\end{aligned}
\end{equation}

Note that Eq. (\ref{eq:smooth_2norm}) is $\frac{1}{\beta}$-smooth.

Using these functions, our final smoothed DC problem is:

\begin{equation}
\label{eq:smooth_dc}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \textrm{LSE}(g_i(\theta), h_i(\theta)) - \frac{1}{n} \sum_{i=1}^{n} h_i(\theta)
\end{aligned}
\end{equation}

The affine version is:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \log\left\{ \exp \left(\| \tilde{A}(\tilde{x}_i - \tilde{y}_i) \|_\beta + \alpha \right) + \exp \left(\| \tilde{A}(\tilde{x}_i - \tilde{z}_i) \|_\beta \right)  \right \} - \frac{1}{n} \sum_{i=1}^{n} \| \tilde{A}(\tilde{x}_i - \tilde{z}_i) \|_\beta
\end{aligned}
\end{equation}

To start, we examine a simpler case where $\tilde{A} = w \in \mathbb{R}^d$ and $n = 1$:

\begin{equation}
\begin{aligned}
    \min_{w} \quad & \log\left\{ \exp \left(\| w^{\top}(x - y) \|_\beta + \alpha \right) + \exp \left(\| w^{\top}(x - z) \|_\beta \right)  \right \} - \| w^{\top}(x - z) \|_\beta
\end{aligned}
\end{equation}

\subsection{Analysis of Gradient Method}

We analyze this formulation using the methodology from \cite{khamaru_convergence_nodate}. To use the theorem proven in that work, we need $g$ to be continuously differentiable and $M_g$-smooth, and we need $h$ to be continuous and convex. These properties are satisfied for affine choice of $f$ in Problem (\ref{eq:smooth_dc}).

Since $h_i(\theta)$ is differentiable for our case, we can use a regular gradient step and achieve the convergence bound for Algorithm 1 in \cite{khamaru_convergence_nodate}. Not sure if it is necessary to constrict $\theta \in \mathcal{C}$ as in \cite{khamaru_convergence_nodate}, but this can be done if necessary.

\subsubsection{Does this formulation have a trivial solution? Or a closed form solution?}

$A = w = 0$ does not minimize this function because of the margin parameter $\alpha$. As for finding a closed form solution, we can see if it is clear from computing the gradient.

\subsubsection{Find smoothness $M_g$ and smoothness $M_h$}

\subsubsection{Attempt improvement of convergence bound in \cite{khamaru_convergence_nodate} using the specific properties of our function}

\subsubsection{Does this smooth version allow for triplets which violate the original objective? If so, can this violation be quantified?}

\section{Potential Next Steps for the Project}

\begin{itemize}
  \item Trying different gradient methods
  \item Use Nesterov momentum as in the Princeton slides
  \item Analyzing stochastic gradient method for the problem
  \item Attempting to analyze the non-smoothed problem
  \item Empirical analysis using toy separable data with two classes
  \item Compare empirical behavior of gradient methods for the relaxed problem vs. the original problem
  \item Compare nearest neighbors performance for difference constraints on the weight space, C
\end{itemize}

\printbibliography

\end{document}
\grid
\grid