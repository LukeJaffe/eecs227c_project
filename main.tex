\documentclass[11pt]{article}

\usepackage[final]{neurips_2019}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}  %hyperref still needs to be put at the end!
%\usepackage[sorting=none]{biblatex}
%\addbibresource{references.bib}
\usepackage{graphicx}

%\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}


\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\DeclareMathOperator*{\argmin}{arg\,min}

%\topmargin -.5in
%\textheight 9in
%\oddsidemargin -.25in
%\evensidemargin -.25in
%\textwidth 7in

\begin{document}


% ========== Edit your name here
\author{Lucas Jaffe}

\title{
    Analysis of Gradient Descent on the Triplet Margin Loss \\
    \large EECS227C Project
    }
\maketitle

\medskip

% In this work we study:
% \begin{itemize}
%     \item Two metric learning objectives
%     \item Relaxation and reformulation of these objectives as DC functions
%     \item Smoothability of these objectives, and study of accuracy/smoothness tradeoff
%     \item Dependence on data of conditioning of the problem
%     \item Study of smoothness parameters vs. minima for original and smooth relaxation
%     \item Interpretations of these objectives as learning a separating hyperplane with soft margin
%     \item Extension of the objectives to learning an affine mapping instead of weight vector
%     \item Different gradient methods for minimization of these objectives
%     \item Possible extensions to the empirical risk minimization setting
% \end{itemize}


% Other stuff:
% \begin{itemize}
%     \item Different learning rate selection including exact line search, backtracking line search
%     \item Nesterov momentum for speedup (for badly conditioned problems?)
% \end{itemize}

\section{Introduction}

% ========== Begin answering questions here 
The goal of this project is to study gradient descent on the triplet margin loss function, a non-convex objective function used in the metric learning domain. The GitHub repository for the project is linked here\footnote{\url{https://github.com/LukeJaffe/eecs227c_project}}.

\textbf{*Note:} This version of the document is incomplete (submitted 05/08/2020) and the complete version will be submitted on 05/09/2020.

\subsection{Background}
 
 Given some data $X \in \mathbb{R}^{n \times m}$ and corresponding labels $Y \in \mathbb{R}^n$, in the supervised metric learning problem, the goal is to find a mapping $\phi: \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{n \times d}$, such that samples with the same label are close together in some metric space, and samples with different labels are further apart. Classically, this type of problem formulation was studied as a dimensionality reduction method, as in \cite{hadsell_dimensionality_2006}, or for k-nearest neighbors classification, as in \cite{weinberger_distance_2009}. 

More recently, metric learning has been combined with deep learning into a highly accurate vehicle for solving three computer vision problems: verification, re-identification, and retrieval. In the image verification problem, the goal is to tell whether two images belong to the same instance. This is commonly used for authentication of identity, either for unlocking a smartphone\footnote{\url{https://support.apple.com/en-us/HT208108}}, or more recently for airport security (\cite{mccartney_are_2019}). In the image re-identification problem, the goal is to search a database for a single matching image to a query image. This is commonly used for re-identification of people (\cite{zheng_scalable_2015}) / faces (\cite{schroff_facenet_2015}), and vehicles (\cite{liu_deep_2016}). In the image retrieval problem, the goal is to return all similar or matching images to a query image. This is used in reverse image search applications, e.g., from Google\footnote{\url{https://support.google.com/websearch/answer/1325808?co=GENIE.Platform\%3DDesktop&hl=en}} and new law enforcement applications including the platform developed by ClearView AI\footnote{\url{https://clearview.ai/}}.

\subsection{Problem setup}

The \textit{triplet margin loss} function is a common metric learning objective function, which has been demonstrated as empirically effective in the deep learning literature, esp. for facial recognition \cite{schroff_facenet_2015}. This is the primary objective function used in the popular OpenFace library (\cite{amos_openface_2016}). Further, this objective has longevity in the literature, having been defended in \cite{hermans_defense_2017}, and recently generalized in \cite{qian_softtriple_2019}.

A triplet is defined as a set of three samples, two of which share the same label, and one of which has a different label. Given some data $X \in \mathbb{R}^{n \times m}$ and corresponding labels $Y \in \mathbb{R}^n$, we define $X' \in \mathbb{R}^{n' \times m}$ as the set of all possible triplets from $X, Y$. For a given triplet $x_i$, we denote the two samples sharing a label as $x_i^a, x_i^p$ (anchor and positive), and the third sample $x_i^n$ (negative). In addition, we define a margin parameter $\alpha \in \mathbb{R}^+$. Then the triplet margin loss can be stated as follows:

\begin{equation}
    \frac{1}{n'} \sum_{i=1}^{n'} \max(0, \| \phi(x_i^a) - \phi(x_i^p)\|_2^2 - \| \phi(x_i^a) - \phi(x_i^n)\|_2^2 + \alpha)
\end{equation}

We note that the triplet margin formulation described here closely follows from \cite{schroff_facenet_2015}.

Observing $\phi$ as a function of some parameter set $\theta$, parameterized by the triplet set $X'$, we can use our objective for empirical risk minimization by solving the following optimization problem:

\begin{equation}
\label{eq:triplet_objective}
\begin{aligned}
    \min_{\theta} \frac{1}{n'} \sum_{i=1}^{n'} \max(0, \| \phi(\theta; x_i^a) - \phi(\theta; x_i^p)\|_2^2 - \| \phi(\theta; x_i^a) - \phi(\theta; x_i^n)\|_2^2 + \alpha)
\end{aligned}
\end{equation}

Conceptually, the goal of this problem is to learn $\theta$ such that samples with the same label are closer together in Euclidean space than samples with different labels. Observing each class cluster as a hypersphere in $\mathbb{R}^d$, achieving a value of $0$ for this objective corresponds to pushing each class hypersphere apart by the max squared diameter of any class hypersphere plus the margin parameter $\alpha$. In practice, this objective has been found to have nice generalization properties for out-of-sample data.

While this objective has seen widespread usage for learning image and word embeddings, there has been no corresponding convergence analysis to date, even for a simple case. The goal of this work is to present an analysis of gradient descent for a simplified smooth version of (\ref{eq:triplet_objective}), with $\phi$: $\phi(w; x) = \langle w, x \rangle$, $w \in \mathbb{R}^{m}$, with $n=1, d=1$. The reasoning for this choice will be presented in the following sections.

\subsection{DC reformulation}

It is clear that (\ref{eq:triplet_objective}) is not convex or differentiable as posed, but we will show that for affine choice of $\phi$, the objective can be reformulated as a difference of convex functions (DC function). Let $A \in \mathbb{R}^{d \times m}$, and let $\phi(A; x) = Ax$. Then we have

\begin{align*}
    f(A; X') &= \frac{1}{n'} \sum_{i=1}^{n'} \max(0, \| Ax_i^a - Ax_i^p\|_2^2 - \| Ax_i^a - Ax_i^n \|_2^2 + \alpha) \\
    &= \frac{1}{n'} \sum_{i=1}^{n'} \max(0, (x_i^a - x_i^p)A^{\top}A(x_i^a - x_i^p) - (x_i^a - x_i^n)A^{\top}A(x_i^a - x_i^n) + \alpha) 
\end{align*}

Let $u_i = x_i^a - x_i^p$ and $v_i = x_i^a - x_i^n$. Using the identity $\max(0, a - b) = \max(a, b) - b$, we have

\begin{align*}
    f(A; X') &= \frac{1}{n'} \sum_{i=1}^{n'} \max(u_i A^{\top}Au_i + \alpha, \; v_i A^{\top}A v_i) - v_i A^{\top}A v_i \\
    &= \frac{1}{n'} \sum_{i=1}^{n'} \max(u_i A^{\top}Au_i + \alpha, \; v_i A^{\top}A v_i) - \frac{1}{n'} \sum_{i=1}^{n'} v_i A^{\top}A v_i
\end{align*}

Since $A^{\top}A \succeq 0$, \; $u_i A^{\top}Au_i \geq 0$ is convex. In addition, we know that a max of convex functions is convex, and a sum of convex functions is convex. Thus, we have a difference of convex functions

\begin{equation}
    f(A; X') = g(A; X') - h(A; X')
\end{equation}

with

\begin{equation}
    g(A; X') = \frac{1}{n'} \sum_{i=1}^{n'} \max(u_i A^{\top}Au_i + \alpha, \; v_i A^{\top}A v_i)
\end{equation}

\begin{equation}
    h(A; X') = \frac{1}{n'} \sum_{i=1}^{n'} v_i A^{\top}A v_i
\end{equation}

We note that there is a large body of literature focused on optimizing functions of the DC form, including \cite{an_dc_2005}, \cite{lipp_variations_2016}, and \cite{khamaru_convergence_2018}.

To simplify the analysis, we focus on the case where $n = 1, d = 1$, and replace the matrix $A$ with the vector $w \in \mathbb{R}^m$. Using this substitution, we have

\begin{equation}
    g(w; X') = \max((w^{\top} u)^2 + \alpha, \; (w^{\top} v)^2)
\end{equation}

\begin{equation}
    h(w; X') = (w^{\top} v)^2
\end{equation}

\subsection{Smooth approximation}

In addition, the objective can be smoothed using the LogSumExp function with parameter $\mu > 0$, which approximates the max function. Defining the LogSumExp function $\textrm{LSE}(x)$, $x \in \mathbb{R}^k$ as

\begin{equation}
    \textrm{LSE}(x; \mu) = \mu \log \sum_{j=1}^{k} \exp(\frac{1}{\mu}x_j)
\end{equation}

This function approximates the max function arbitrarily closely as $\mu$ approaches $0$

\begin{equation}
    \lim_{\mu \to 0^+}  \textrm{LSE}(x; \mu) = \max_j x_j
\end{equation}

Using this approximation, we can rewrite the first term of our DC reformulation as

\begin{equation}
    g(w; X') = \mu \log(\exp(\frac{1}{\mu}((w^{\top} u)^2) + \alpha), \; \exp(\frac{1}{\mu}(w^{\top} v)^2))
\end{equation}

For clarity, we write out the full function in this form as well

\begin{equation}
\label{eq:full_smooth_margin_f}
    f(w; X') = \mu \log(\exp(\frac{1}{\mu}((w^{\top} u)^2) + \alpha), \; \exp(\frac{1}{\mu}(w^{\top} v)^2)) - (w^{\top} v)^2
\end{equation}

\subsection{Outline}

\section{Results}

For our initial analysis, we observe the case where $\mu = 1$ and $\alpha = 0$.

\begin{equation}
    g(w; X') = \log(\exp((w^{\top} u)^2), \; \exp((w^{\top} v)^2))
\end{equation}

\subsection{Notation}

In this section, we clarify notation which will be utilized in following sections.

The notation $xx^{\top}$ is used frequently, referring to the dyadic matrix which is an outer product of some vector $x \in \mathbb{R}^m$ with itself. This matrix is rank one and positive semi-definite, with its only nonzero eigenvalue $\lambda_{\max}(xx^{\top}) = x^{\top}x = \|x\|_2^2$.

\subsection{Smoothness of composite functions}

We begin with an analysis of the smoothness of the quadratic function $(w^{\top} x)^2)$ and the LogSumExp function, to help interpret later results.

We have that

\begin{equation}
    \nabla_w^2 ((w^{\top} x)^2) = xx^{\top}
\end{equation}

Meaning that $(w^{\top} x)^2$ is $M$-smooth, with $M = \lambda_{\max}(xx^{\top}) = \|x\|_2^2$.

Analyzing the LogSumExp function, we have that

\begin{equation}
    \nabla^2 \textrm{LSE}(w; \mu) = 
\end{equation}

A proof of these facts can be found in \cite{gao_properties_2018}.

This section will be finished shortly.

\subsection{Analysis of Full Function Minima}

The function $f(w; u, v)$ has two possible minima, which are dependent on the data $u, v$. Let $B = uu^{\top} - vv^{\top}$. Then we have

\begin{equation}
    f(w; B) = \log(\exp(w^{\top}Bw) + 1)
\end{equation}

Since $w^{\top}Bw$ has the form of a standard quadratic in $w$, we have

\begin{equation}
    \min_{w} w^{\top}Bw = \begin{cases} 
    0 &\mbox{if } B \succeq 0 \\
    -\infty & \mbox{otherwise } \end{cases}
\end{equation}

Extending this to $f(w; B)$, we have

\begin{equation}
    \inf_{w} f(w; B) = \begin{cases} 
    \log(2) &\mbox{if } B \succeq 0 \\
    0 & \mbox{otherwise } \end{cases}
\end{equation}

We note that $B \succeq 0$ will occur if and only if $u, v$ are linearly dependent.

In Figures 1 and 2, we show the triplet margin loss as posed in (\ref{eq:full_smooth_margin_f}), with $\alpha=0$ and $\alpha=0.1$ respectively. We note that the randomly generated sample gives $f^*=0$ for both cases.

\begin{figure}
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=\textwidth]{figures/triplet_viz_alpha0.png}
  \label{fig:triplet_alpha0}
  \caption{This figure shows the triplet margin loss with $\alpha = 0$ for a randomly generated triplet sample. The leftmost figure shows the original objective with no smoothing. The center figure shows the smooth objective with $\mu=0.1$. The rightmost figure shows the smooth objective with $\mu=1$. }
\end{figure}

\begin{figure}
  \centering
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \includegraphics[width=\textwidth]{figures/triplet_viz.png}
  \caption{This figure is identical to Figure 1, but with $\alpha = 0.1$.}
\end{figure}

\subsection{Smoothness Analysis of $g(w)$}

\begin{equation}
    g(w) = \log( \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) )
\end{equation}

\begin{equation}
    \nabla g(w) 
    = \frac{ 2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} w}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{top}) 
    &= 4 \{ \exp((w^{\top}u)^2)uu^{\top}ww^{\top}uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}ww^{\top}vv^{\top} \} \\
    &= 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \}
\end{align*}

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{bot}) = \frac{ -2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top} \} w }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 g(w) &= 
    \frac{ 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \} }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \\
    &- \frac{ 4 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} ww^{\top} \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Re-arranging, we have:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \left\{ \frac{  \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } +
    \frac{  \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\} \\
    &- 4 \left\{
    \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\}
    ww^{\top} \\
     & \left\{ \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  }
     \right\}
\end{align*}

We can simplify further by using the substitution: 
\begin{align*}
p = \frac{\exp((w^{\top}u)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Note that $0 < p < 1$, and:
\begin{align*}
1 - p = \frac{\exp((w^{\top}v)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Plugging in to the Hessian expression:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \{ p(w^{\top}u)^2 uu^{\top} + (1-p)(w^{\top}v)^2 vv^{\top} \} \\
    &- 4 \{ p uu^{\top} + (1-p) vv^{\top} \}ww^{\top}\{ p uu^{\top} + (1-p) vv^{\top} \} \\
    &= 4 p(w^{\top}u)^2 uu^{\top} + 4 (1-p)(w^{\top}v)^2 vv^{\top} \\
    &- 4 p^2 (w^{\top}u)^2 uu^{\top} - 4 (1-p)^2 (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \\
    &= 4 p (1 - p) (w^{\top}u)^2 uu^{\top} + 4 p (1-p) (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top})
\end{align*}

Using $0 < p(1-p) \leq \frac{1}{4}$, we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
    z^{\top} \nabla^2 g(w) z &\leq
    z^{\top} \left\{ (w^{\top}u)^2 uu^{\top} + (w^{\top}v)^2 vv^{\top} - 2 (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \right\} z \\
    &\leq \|z\|_2^2 \left\{ (w^{\top}u)^2 \|u\|_2^2 + (w^{\top}v)^2 \|v\|_2^2 + 4 \left| (w^{\top}u)(w^{\top}v)(u^{\top}v) \right| \right\} \\
    &\leq \|z\|_2^2 \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 g(w) \preceq \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\} I_d
\end{equation}

Meaning that $g(w)$ is $M_g$-smooth, with $M_g = \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Smoothness Analysis of Full Function}

In this section, we analyze a simplified version of the target function, with $\mu = 1$ and $\alpha = 0$. We simplify notation by letting $u = x - y$ and $v = x - z$.

\begin{equation}
    f(w) = \log(\exp((w^{\top}u)^2 - (w^{\top}v)^2) + 1)
\end{equation}

We can simplify further by letting $B = uu^\top - vv^\top$:

\begin{equation}
\label{eq:f}
    f(w) = \log(\exp(w^{\top}Bw) + 1)
\end{equation}

\begin{equation}
    \nabla f(w) = \frac{2 \exp(w^{\top}Bw)Bw}{\exp(w^{\top}Bw) + 1}
\end{equation}

We can simplify the notation using the substitution

\begin{equation}
q = \frac{\exp(w^{\top}Bw)}{\exp(w^{\top}Bw) + 1}
\end{equation}

Applying this substitution, we have

\begin{equation}
\label{eq:grad_f}
    \nabla f(w) = 2qBw
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{top}) = 4 \exp(w^{\top}Bw)Bww^{\top}B  + 2 \exp(w^{\top}Bw)B
\end{align*}

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{bot}) = \frac{-2 \exp(w^{\top}Bw)B}{\exp(w^{\top}Bw) + 1}
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 f(w) = \frac{4 \exp(w^{\top}Bw)B(ww^{\top}B + \frac{1}{2}I_d)}{\exp(w^{\top}Bw) + 1} - \frac{4 \{\exp(w^{\top}Bw)\}^2 Bww^{\top}B }{\{\exp(w^{\top}Bw) + 1\}^2}
\end{align*}

Using this substitution of $q$,  we have:
\begin{equation}
    \nabla^2 f(w) = 4q(1-q)Bww^{\top}B + 2qB
\end{equation}

Noting that $0 < q < 1$ and $0 < q^2 \leq \frac{1}{4}$ we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
z^{\top} \nabla^2 f(w) z 
&= 4q(1-q) z^{\top} Bww^{\top}B z + 2q z^{\top} B z \\
&< z^{\top} Bww^{\top}B z + 2 z^{\top} B z \\
&\leq \|z\|_2^2 \|w\|_2^2 \|B\|_2^2 + 2 \|z\|_2^2 \|B\|_2 \\
&= \|z\|_2^2 ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) 
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 f(w) \prec ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) I_d
\end{equation}

Meaning that $f(w)$ is $M_f$-smooth, with $M_f = \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2$.

\subsection{Gradient descent analysis of full function}

We analyze the standard gradient step

\begin{equation}
    w^{t+1} = w^t - \eta \nabla f(w^t)
\end{equation}

We would like to show that the following descent condition holds for some $\eta > 0, c < 0$

\begin{equation}
\label{eq:descent_condition}
    f(w^{t+1}) \leq f(w^t) + c
\end{equation}

This property is not guaranteed in general for a difference of convex functions, and we cannot rely on properties of convexity that would typically be applied. Fortunately, this function has special structure as we will show in the following derivation.

\begin{equation}
    f(w^{t+1}) = f(w^t - \eta \nabla f(w^t)) 
\end{equation}

Using $\nabla f(w))$ from Eq. (\ref{eq:grad_f}) , we have

\begin{align*}
    w - \eta \nabla f(w) &= w - 2qBw \\
    &= (I_d - 2qB)w
\end{align*}

Applying this term to $f$ from (\ref{eq:f}), we get

\begin{equation}
\begin{split}
    f(w^t - \eta \nabla f(w^t)) 
    &= \log(\exp(w^{\top}(I_d - 2 \eta qB)B(I_d - 2 \eta qB)w) + 1) \\
    &= \log(\exp(w^{\top}Bw + w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w) + 1)
\end{split}
\end{equation}

Let $b_\eta = w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w$. Making this substitution, we have

\begin{equation}
\begin{split}
    f(w^t - \eta \nabla f(w^t)) 
    &= \log(\exp(w^{\top}Bw + b_\eta ) + 1) \\
    &= \log(\exp(w^{\top}Bw)\exp(b_\eta ) + 1)
\end{split}
\end{equation}

Examining the desired descent condition from (\ref{eq:descent_condition}), we want to find some $c < 0$ such that

\begin{align*}
    f(w^{t+1}) - f(w^t) \leq c
\end{align*}

Substituting in, we find that

\begin{align*}
    f(w^{t+1}) - f(w^t) &= 
    \log(\exp(w^{\top}Bw)\exp(b_\eta ) + 1) - \log(\exp(w^{\top}Bw) + 1) \\
    &= \log \left\{ \frac{\exp(w^{\top}Bw)\exp(b_\eta ) + 1}{\exp(w^{\top}Bw) + 1} \right\} \\
    &= \log( q \exp(b_\eta ) + (1 - q))
\end{align*}

Therefore, we must select $c < \log( q \exp(b_\eta ) + (1 - q))$ to satisfy the descent condition. In particular, this form shows that the descent condition holds for $b_\eta < 0$.

\subsection{Selecting the step size $\eta$}

Since we have that $f(w^{t+1}) < f(w^t) + c, \; c < 0$ we would like to select the step size $\eta$ such that $c$ is minimized at each step. Using the direct line search approach, we attempt to solve the following optimization problem:

\begin{equation}
    c^* = \min_{\eta} \log( q \exp(b_\eta ) + (1 - q))
\end{equation}

In terms of $\eta$, we want to find

\begin{equation}
\label{eq:eta_opt}
\begin{split}
    \eta^* &= \argmin_{\eta} \log( q \exp(b_\eta ) + (1 - q)) \\
    &= \argmin_{\eta} b_\eta \\
    &= \argmin_{\eta} w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w
\end{split}
\end{equation}

% Solve two cases: one where B is PSD, the other where it isn't
Recall that $B \nsucceq 0$, which means that problem (\ref{eq:eta_opt}) is not convex. We will show that a good choice of $\eta$ is $\eta = \frac{1}{\|B\|_2}$.

\begin{align*}
    b_\eta &= w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w \\
    &= \frac{4 q^2 w^{\top} B^3 w}{\|B\|_2^2} - \frac{4 q w^{\top} B^2 w}{\|B\|_2} \\
    &\leq \frac{4 q^2 \|B\|_2^3}{\|B\|_2^2} - \frac{4 q \|B\|_2^2 }{\|B\|_2} \\
    &= 4 q (q-1) \|B\|_2 \\
    &\implies -\|B\|_2 \leq 4 q (q-1) \|B\|_2 \leq 0
\end{align*}

The last step using $-\frac{1}{4} \leq q (q-1) \leq 0$. Since the given choice of $\eta$ is the largest valid choice which guarantees $b_\eta \leq 0$, it is optimal.

\subsection{Bound on the iterate}

This section will be added shortly.

\subsection{Convergence of the squared gradient norm}

In \cite{khamaru_convergence_2018}, the following bound is given on the average squared gradient norm, with $\eta \in (0, \frac{1}{M_g}]$

\begin{equation}
    \textrm{Avg}(\|\nabla f(x^k) \|_2^2) \leq \frac{2(f(x^0) - f^*)}{\eta(k + 1)}
\end{equation}

We would like to attempt improvement of this bound using the specific properties of our function. This section will be added shortly.

\subsection{Convergence of the function value}

Since we have $f(w^{t+1}) < f(w^t) + c$, we can attempt to show convergence of the function value. This section will be added shortly.

%\subsection{Experiments}

%\subsection{Extension to case with $\mu \neq 1$ and $\alpha > 0$}

%\subsection{Extension to case with $m > 1$}

%\subsection{Analysis with bias term}

%To simplify analysis of gradient methods, we will concatenate $A$ and $b$ and append a $1$ to $x$: $\tilde{A} = [A, b]$, $\tilde{x}^\top = [x^\top, 1]$. Now we have $f(\tilde{A}; \tilde{x}) = \tilde{A} \tilde{x}$, $\tilde{A} \in \mathbb{R}^{d \times m+1}$.

% \subsection{Analysis of Gradient Method}

% We analyze this formulation using the methodology from \cite{khamaru_convergence_2018}. To use the theorem proven in that work, we need $g$ to be continuously differentiable and $M_g$-smooth, and we need $h$ to be continuous and convex. These properties are satisfied for affine choice of $f$ in Problem (\ref{eq:smooth_dc}).

% Since $h_i(\theta)$ is differentiable for our case, we can use a regular gradient step and achieve the convergence bound for Algorithm 1 in \cite{khamaru_convergence_2018}. Not sure if it is necessary to constrict $\theta \in \mathcal{C}$ as in \cite{khamaru_convergence_2018}, but this can be done if necessary.

% $A = w = 0$ does not minimize this function because of the margin parameter $\alpha$. As for finding a closed form solution, we can see if it is clear from computing the gradient.

%\subsubsection{Does this smooth version allow for triplets which violate the original objective? If so, can this violation be quantified?}

\section{Conclusion}

\subsection{Discussion}

In this work, we show that the triplet margin loss function can be reformulated as a difference of convex functions, and that a reasonable smooth approximation can be formulated with the LogSumExp function. We show that this function is smooth, and give an upper bound for the smoothness which is dependent on the iterate. Then, we attempt a convergence analysis for the iterate, the squared gradient norm, and for the function value. While none of the convergence bounds achieved were very good, they lay a nice foundation for future work.

One takeaway from this effort is that the condition of convexity cannot be trivially replaced just by knowing the function structure. 

It would be interesting to see if different algorithms, including a proximal type alforithm, Frank-Wolfe, or the stochastic gradient method could be easier to anlalyze.

We would also like to test if the functions satisfies the Kurdaya-Åojasiewicz, as detailed in \cite{khamaru_convergence_2018}, to achieve a faster convergence rate.

\subsection{Future Work}

There are several extensions that we would like to perform on this analysis. First, we would like to repeat the analysis for $\alpha > 0$ and $\mu \neq 1$. Second we would like to analyze the multi-sample case ($n > 1$). Third we would like to study the case where $d > 1$. Fourth, we would like to add a learnable bias term to the analysis. Finally, we would like to study the stochastic case, so that this work can be applied in practice for empirical risk minimization, given randomly sampled data.

One potential application of this analysis could be to ``warm-start'' the last layer of a neural network.

We would also like to compare the function to the soft margin loss and logistic loss for the support vector machine (SVM). It would be interesting to compare the learned weights $w$, and see how the models compare in terms of generalization.

% \subsection{Previously wanted next steps}

% \begin{itemize}
%   \item Trying different gradient methods
%   \item Use Nesterov momentum as in the Princeton slides
%   \item Analyzing stochastic gradient method for the problem
%   \item Attempting to analyze the non-smoothed problem
%   \item Empirical analysis using toy separable data with two classes
%   \item Compare empirical behavior of gradient methods for the relaxed problem vs. the original problem
%   \item Compare nearest neighbors performance for difference constraints on the weight space, C
% \end{itemize}

\bibliography{references}

\end{document}
\grid
\grid