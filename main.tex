\documentclass[11pt]{article}

\usepackage[final]{neurips_2019}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage[sorting=none]{biblatex}
%\addbibresource{references.bib}

%\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}


\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy
\DeclareMathOperator*{\argmin}{arg\,min}

%\topmargin -.5in
%\textheight 9in
%\oddsidemargin -.25in
%\evensidemargin -.25in
%\textwidth 7in

\begin{document}


% ========== Edit your name here
\author{Lucas Jaffe}

\title{
    EECS227C Project \\
    \large Convergence Analysis for Gradient Descent on the Triplet Margin Loss
    }
\maketitle

\medskip

\section{Introduction}

% ========== Begin answering questions here 
The \textit{triplet margin loss} function is a common \textit{metric learning} objective function, which has been demonstrated as empirically effective in the deep learning literature \cite{schroff_facenet_2015, hermans_defense_2017}. Given some data $X \in \mathbb{R}^{n \times m}$ and corresponding labels $Y \in \mathbb{R}^n$, in the supervised metric learning problem, the goal is to find a mapping $f: \mathbb{R}^{n \times m} \rightarrow \mathbb{R}^{n \times d}$ on $X$, such that samples with the same label are close together in some metric space, and samples with different labels are further apart. Classically, this type of problem formulation was commonly studied as a dimensionality reduction method, as in \cite{hadsell_dimensionality_2006}, or for k-nearest neighbors classification. 

More recently, metric learning has been combined with deep learning into a highly accurate vehicle for three computer vision problems: verification, re-identification, and retrieval. In the image verification problem, the goal is to tell whether two images belong to the same instance. This is commonly used for authentication of identity, either for unlocking a smartphone (cite apple), or more recently for airport security (cite wsj). In the image re-identification problem, the goal is to search a database for a single matching image to a query image. This is commonly used for re-identification of people / faces (cite facenet), and vehicles (cite vehicle id). In the image retrieval problem, the goal is to return all similar or matching images to a query image. This is used in reverse image search applications, as in (cite google, tineye).

Let $X, Y, Z \in \mathbb{R}^{n \times m}$ be a collection of data, with samples $x_i, y_i, z_i \in \mathbb{R}^m$ being rows from each matrix respectively, such that samples from $X$ and $Y$ have the same label, and samples from $Z$ have a different label. Let $f: \mathbb{R}^m \rightarrow \mathbb{R}^d$ be an \textit{embedding function} which projects samples into an \textit{embedding space}, $\mathbb{R}^d$. Let $\alpha \in \mathbb{R}^+$ be the margin parameter. Then the triplet margin loss can be stated as follows:

\begin{equation}
    \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(x_i) - f(y_i)\|_2^2 - \| f(x_i) - f(z_i)\|_2^2 + \alpha)
\end{equation}

Let $f$ be parameterized by some parameter set $\theta \in \Theta$, and restrict $\| f(\theta)\|_2 = 1 \quad \forall \theta \in \Theta$. Then we can use our objective for empirical risk minimization by solving the following optimization problem:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 + \alpha) \\
    \textrm{s.t.} \quad &  \| f(\theta)\|_2 = 1 \quad \forall \theta \in \Theta
\end{aligned}
\end{equation}

While this objective has seen widespread usage for learning image and word embeddings, there has been no corresponding convergence analysis to date, even for a simple case. The goal of this work is to present a convergence analysis for an affine choice of $f$: $f(A, b; x) = Ax + b$, $A \in \mathbb{R}^{d \times m}, b \in \mathbb{R}^d$. To simplify analysis of gradient methods, we will concatenate $A$ and $b$ and append a $1$ to $x$: $\tilde{A} = [A, b]$, $\tilde{x}^\top = [x^\top, 1]$. Now we have $f(\tilde{A}; \tilde{x}) = \tilde{A} \tilde{x}$, $\tilde{A} \in \mathbb{R}^{d \times m+1}$.

Then we can simplify the optimization problem as follows:

\begin{flalign*}
& \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 & \\
& = \| f(\theta; x_i) \|_2^2 + \| f(\theta; y_i) \|_2^2 - 2\langle f(\theta; x_i), f(\theta; y_i) \rangle - \| f(\theta; x_i) \|_2^2 - \| f(\theta; z_i) \|_2^2 + 2\langle f(\theta; x_i), f(\theta; z_i) \rangle \\
& = \| f(\theta; y_i) \|_2^2 - \| f(\theta; z_i) \|_2^2 + 2\langle f(\theta; x_i), f(\theta; z_i) - f(\theta; y_i) \rangle \\
& = 2\langle f(\theta; x_i), f(\theta; z_i) - f(\theta; y_i) \rangle \quad\text{(using $\| f(\theta)\|_2 = 1$)} \\
\end{flalign*}

Now substituting affine choice of $f$:

\begin{flalign*}
& = 2\langle f(\tilde{A}; \tilde{x_i}), f(\tilde{A}; \tilde{z_i}) - f(\tilde{A}; \tilde{y_i}) \rangle & \\
& = 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i})
\end{flalign*}

And our optimization problem can be written as:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha) \\
    \textrm{s.t.} \quad &  \| f(\tilde{A})\|_2 = 1
\end{aligned}
\end{equation}

We note that this problem is convex, but not smooth.

\bigskip

***\textbf{Note}: Actually, I am not sure if the problem can be written that way, since the constraint $\| f(\theta)\|_2 = 1$ is obtained in practice by dividing $f$ by its norm: $\| g(\theta)\|_2 = \frac{f(\theta)}{\| f(\theta)\|_2} \rightarrow \| g(\theta)\|_2 = 1$. In this case, the problem would look like:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \max\left(0, 2\left\langle \frac{\tilde{A} \tilde{x_i}}{\| \tilde{A} \tilde{x_i} \|_2}, \frac{\tilde{A} \tilde{z_i}}{\| \tilde{A} \tilde{z_i} \|_2} - \frac{\tilde{A} \tilde{y_i}}{\| \tilde{A} \tilde{y_i} \|_2} \right\rangle + \alpha\right)
\end{aligned}
\end{equation}

I am not sure if this can be reformulated as a convex problem.

In this work we study:
\begin{itemize}
    \item Two metric learning objectives
    \item Relaxation and reformulation of these objectives as DC functions
    \item Smoothability of these objectives, and study of accuracy/smoothness tradeoff
    \item Dependence on data of conditioning of the problem
    \item Study of smoothness parameters vs. minima for original and smooth relaxation
    \item Interpretations of these objectives as learning a separating hyperplane with soft margin
    \item Extension of the objectives to learning an affine mapping instead of weight vector
    \item Different gradient methods for minimization of these objectives
    \item Possible extensions to the empirical risk minimization setting
\end{itemize}


Other stuff:
\begin{itemize}
    \item Different learning rate selection including exact line search, backtracking line search
    \item Nesterov momentum for speedup (for badly conditioned problems?)
\end{itemize}

\section{Initial writeup for simplified problem}

\subsection{Notation}

\subsection{DC Reformulation}

\subsection{Analysis of Full Function Minima}

The function $f(w; u, v)$ has two possible minima, which are dependent on the data $u, v$. Let $B = uu^{\top} - vv^{\top}$. Then we have

\begin{equation}
    f(w; B) = \log(\exp(w^{\top}Bw) + 1)
\end{equation}

Since $w^{\top}Bw$ has the form of a standard quadratic in $w$, we have

\begin{equation}
    \min_{w} w^{\top}Bw = \begin{cases} 
    0 &\mbox{if } B \succeq 0 \\
    -\infty & \mbox{otherwise } \end{cases}
\end{equation}

Extending this to $f(w; B)$, we have

\begin{equation}
    \inf_{w} f(w; B) = \begin{cases} 
    \log(2) &\mbox{if } B \succeq 0 \\
    0 & \mbox{otherwise } \end{cases}
\end{equation}

We note that $B \succeq 0$ will occur if and only if $u, v$ are linearly dependent.

\subsection{Smoothness analysis of composite functions}

Show smoothness for LogSumExp, quadratic, to give intuition for following results

\subsection{Smoothness Analysis of $g(w)$}

\begin{equation}
    g(w) = \log( \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) )
\end{equation}

\begin{equation}
    \nabla g(w) 
    = \frac{ 2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} w}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{top}) 
    &= 4 \{ \exp((w^{\top}u)^2)uu^{\top}ww^{\top}uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}ww^{\top}vv^{\top} \} \\
    &= 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \}
\end{align*}

\begin{align*}
    \nabla (\nabla g(w) \; \textrm{bot}) = \frac{ -2 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top} \} w }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 g(w) &= 
    \frac{ 4 \{ \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top} + \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top} \} }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \\
    &- \frac{ 4 \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} ww^{\top} \{ \exp((w^{\top}u)^2)uu^{\top} + \exp((w^{\top}v)^2)vv^{\top}  \} }{ \{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) \}^2 }
\end{align*}

Re-arranging, we have:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \left\{ \frac{  \exp((w^{\top}u)^2)(w^{\top}u)^2 uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } +
    \frac{  \exp((w^{\top}v)^2)(w^{\top}v)^2 vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\} \\
    &- 4 \left\{
    \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } \right\}
    ww^{\top} \\
     & \left\{ \frac{  \exp((w^{\top}u)^2) uu^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  } + \frac{  \exp((w^{\top}v)^2) vv^{\top}  }{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2)  }
     \right\}
\end{align*}

We can simplify further by using the substitution: 
\begin{align*}
p = \frac{\exp((w^{\top}u)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Note that $0 < p < 1$, and:
\begin{align*}
1 - p = \frac{\exp((w^{\top}v)^2)}{ \exp((w^{\top}u)^2) + \exp((w^{\top}v)^2) }
\end{align*}

Plugging in to the Hessian expression:

\begin{align*}
    \nabla^2 g(w) &= 
    4 \{ p(w^{\top}u)^2 uu^{\top} + (1-p)(w^{\top}v)^2 vv^{\top} \} \\
    &- 4 \{ p uu^{\top} + (1-p) vv^{\top} \}ww^{\top}\{ p uu^{\top} + (1-p) vv^{\top} \} \\
    &= 4 p(w^{\top}u)^2 uu^{\top} + 4 (1-p)(w^{\top}v)^2 vv^{\top} \\
    &- 4 p^2 (w^{\top}u)^2 uu^{\top} - 4 (1-p)^2 (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \\
    &= 4 p (1 - p) (w^{\top}u)^2 uu^{\top} + 4 p (1-p) (w^{\top}v)^2 vv^{\top} \\
    &- 8 p (1-p) (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top})
\end{align*}

Using $0 < p(1-p) \leq \frac{1}{4}$, we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
    z^{\top} \nabla^2 g(w) z &\leq
    z^{\top} \left\{ (w^{\top}u)^2 uu^{\top} + (w^{\top}v)^2 vv^{\top} - 2 (w^{\top}u)(w^{\top}v)(uv^{\top} + vu^{\top}) \right\} z \\
    &\leq \|z\|_2^2 \left\{ (w^{\top}u)^2 \|u\|_2^2 + (w^{\top}v)^2 \|v\|_2^2 + 4 \left| (w^{\top}u)(w^{\top}v)(u^{\top}v) \right| \right\} \\
    &\leq \|z\|_2^2 \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 g(w) \preceq \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\} I_d
\end{equation}

Meaning that $g(w)$ is $M_g$-smooth, with $M_g = \|w\|_2^2 \left\{ \|u\|_2^4 + \|v\|_2^4 + 4 \|u\|_2^2 \|v\|_2^2 \right\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Smoothness Analysis of Full Function}

In this section, we analyze a simplified version of the target function, with $\mu = 1$ and $\alpha = 0$. We simplify notation by letting $u = x - y$ and $v = x - z$.

\begin{equation}
    f(w) = \log(\exp((w^{\top}u)^2 - (w^{\top}v)^2) + 1)
\end{equation}

We can simplify further by letting $B = uu^\top - vv^\top$:

\begin{equation}
\label{eq:f}
    f(w) = \log(\exp(w^{\top}Bw) + 1)
\end{equation}

\begin{equation}
    \nabla f(w) = \frac{2 \exp(w^{\top}Bw)Bw}{\exp(w^{\top}Bw) + 1}
\end{equation}

We can simplify the notation using the substitution

\begin{equation}
q = \frac{\exp(w^{\top}Bw)}{\exp(w^{\top}Bw) + 1}
\end{equation}

Applying this substitution, we have

\begin{equation}
\label{eq:grad_f}
    \nabla f(w) = 2qBw
\end{equation}

To find the Hessian, we break the gradient into two parts for use of the chain rule:

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{top}) = 4 \exp(w^{\top}Bw)Bww^{\top}B  + 2 \exp(w^{\top}Bw)B
\end{align*}

\begin{align*}
    \nabla (\nabla f(w) \; \textrm{bot}) = \frac{-2 \exp(w^{\top}Bw)B}{\exp(w^{\top}Bw) + 1}
\end{align*}

Combining the parts:

\begin{align*}
    \nabla^2 f(w) = \frac{4 \exp(w^{\top}Bw)B(ww^{\top}B + \frac{1}{2}I_d)}{\exp(w^{\top}Bw) + 1} - \frac{4 \{\exp(w^{\top}Bw)\}^2 Bww^{\top}B }{\{\exp(w^{\top}Bw) + 1\}^2}
\end{align*}

Using this substitution of $q$,  we have:
\begin{equation}
    \nabla^2 f(w) = 4q(1-q)Bww^{\top}B + 2qB
\end{equation}

Noting that $0 < q < 1$ and $0 < q^2 \leq \frac{1}{4}$ we have, $\forall z \in \mathbb{R}^d$:

\begin{align*}
z^{\top} \nabla^2 f(w) z 
&= 4q(1-q) z^{\top} Bww^{\top}B z + 2q z^{\top} B z \\
&< z^{\top} Bww^{\top}B z + 2 z^{\top} B z \\
&\leq \|z\|_2^2 \|w\|_2^2 \|B\|_2^2 + 2 \|z\|_2^2 \|B\|_2 \\
&= \|z\|_2^2 ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) 
\end{align*}

This gives us:

\begin{equation}
    \nabla^2 f(w) \prec ( \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2 ) I_d
\end{equation}

Meaning that $f(w)$ is $M_f$-smooth, with $M_f = \|w\|_2^2 \|B\|_2^2 + 2 \|B\|_2$.

\subsection{Gradient descent analysis of full function}

We analyze the standard gradient step

\begin{equation}
    w^{t+1} = w^t - \eta \nabla f(w^t)
\end{equation}

We would like to show that the following descent condition holds for some $\eta > 0, c < 0$

\begin{equation}
\label{eq:descent_condition}
    f(w^{t+1}) \leq f(w^t) + c
\end{equation}

This property is not guaranteed in general for a difference of convex functions, and we cannot rely on properties of convexity that would typically be applied. Fortunately, this function has special structure as we will show in the following derivation.

\begin{equation}
    f(w^{t+1}) = f(w^t - \eta \nabla f(w^t)) 
\end{equation}

Using $\nabla f(w))$ from Eq. (\ref{eq:grad_f}) , we have

\begin{align*}
    w - \eta \nabla f(w) &= w - 2qBw \\
    &= (I_d - 2qB)w
\end{align*}

Applying this term to $f$ from (\ref{eq:f}), we get

\begin{equation}
\begin{split}
    f(w^t - \eta \nabla f(w^t)) 
    &= \log(\exp(w^{\top}(I_d - 2 \eta qB)B(I_d - 2 \eta qB)w) + 1) \\
    &= \log(\exp(w^{\top}Bw + w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w) + 1)
\end{split}
\end{equation}

Let $b_\eta = w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w$. Making this substitution, we have

\begin{equation}
\begin{split}
    f(w^t - \eta \nabla f(w^t)) 
    &= \log(\exp(w^{\top}Bw + b_\eta ) + 1) \\
    &= \log(\exp(w^{\top}Bw)\exp(b_\eta ) + 1)
\end{split}
\end{equation}

Examining the desired descent condition from (\ref{eq:descent_condition}), we want to find some $c < 0$ such that

\begin{align*}
    f(w^{t+1}) - f(w^t) \leq c
\end{align*}

Substituting in, we find that

\begin{align*}
    f(w^{t+1}) - f(w^t) &= 
    \log(\exp(w^{\top}Bw)\exp(b_\eta ) + 1) - \log(\exp(w^{\top}Bw) + 1) \\
    &= \log \left\{ \frac{\exp(w^{\top}Bw)\exp(b_\eta ) + 1}{\exp(w^{\top}Bw) + 1} \right\} \\
    &= \log( q \exp(b_\eta ) + (1 - q))
\end{align*}

Therefore, we must select $c < \log( q \exp(b_\eta ) + (1 - q))$ to satisfy the descent condition. In particular, this form shows that the descent condition holds for $b_\eta < 0$.

\subsection{Selecting the step size $\eta$}

Since we have that $f(w^{t+1}) < f(w^t) + c, \; c < 0$ we would like to select the step size $\eta$ such that $c$ is minimized at each step. Using the direct line search approach, we attempt to solve the following optimization problem:

\begin{equation}
    c^* = \min_{\eta} \log( q \exp(b_\eta ) + (1 - q))
\end{equation}

In terms of $\eta$, we want to find

\begin{equation}
\label{eq:eta_opt}
\begin{split}
    \eta^* &= \argmin_{\eta} \log( q \exp(b_\eta ) + (1 - q)) \\
    &= \argmin_{\eta} b_\eta \\
    &= \argmin_{\eta} w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w
\end{split}
\end{equation}

% Solve two cases: one where B is PSD, the other where it isn't
Recall that $B \nsucceq 0$, which means that problem (\ref{eq:eta_opt}) is not convex. We will show that a good choice of $\eta$ is $\eta = \frac{1}{\|B\|_2}$.

\begin{align*}
    b_\eta &= w^{\top} ( 4 \eta^2 q^2 B^3 - 4 \eta q B^2)w \\
    &= \frac{4 q^2 w^{\top} B^3 w}{\|B\|_2^2} - \frac{4 q w^{\top} B^2 w}{\|B\|_2} \\
    &\leq \frac{4 q^2 \|B\|_2^3}{\|B\|_2^2} - \frac{4 q \|B\|_2^2 }{\|B\|_2} \\
    &= 4 q (q-1) \|B\|_2 \\
    &\implies -\|B\|_2 \leq 4 q (q-1) \|B\|_2 \leq 0
\end{align*}

The last step using $-\frac{1}{4} \leq q (q-1) \leq 0$. Since the given choice of $\eta$ is the largest valid choice which guarantees $b_\eta \leq 0$, it is optimal.

\subsection{Convergence analysis using descent condition}

Since we have $f(w^{t+1}) < f(w^t) + c$, and $c$ is a constant which does not depend on the iterate, we get a very simple convergence bound. Let

\begin{align*}
    T \leq \frac{f(w^{t+1}) - f(w^*)}{c}
\end{align*}

\subsection{Extension to case with $\mu \neq 1$ and $\alpha > 0$}

\subsection{Extension to case with $m > 1$}

\subsection{Analysis with bias term}

\subsection{Comparison to logistic loss for SVM in terms of hyperplane boundary learned}


\section{Analysis of Relaxed Problem}

To alleviate issues with the previous formulation, including non-smoothness of the max function, and the norm constraint, we can relax the problem to be easier to analyze, while retaining its high-level properties. Specifically, we will swap the max function for the softplus function, and remove the norm constraint altogether.

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \log{(1 + \exp{( \| f(\theta; x_i) - f(\theta; y_i)\|_2^2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2^2 + \alpha)})}
\end{aligned}
\end{equation}

With affine choice of f:

\begin{equation}
\label{eq:smooth_problem}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \log{(1 + \exp{(\tilde{y_i}^\top\tilde{A}^\top\tilde{A}\tilde{y_i} - \tilde{z_i}^\top\tilde{A}^\top\tilde{A}\tilde{z_i} + 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha)})}
\end{aligned}
\end{equation}

To check convexity of this function, we only need to study the inner portion:

\begin{equation}
\begin{aligned}
g(\tilde{A}) = \tilde{y_i}^\top\tilde{A}^\top\tilde{A}\tilde{y_i} - \tilde{z_i}^\top\tilde{A}^\top\tilde{A}\tilde{z_i} + 2\tilde{x_i}^\top\tilde{A}^\top\tilde{A}(\tilde{z_i} - \tilde{y_i}) + \alpha
\end{aligned}
\end{equation}

We can check if the Hessian of this function is PSD:

\begin{align*}
\nabla{g(\tilde{A})}
&= 2\tilde{A}\tilde{y_i}\tilde{y_i}^\top - 2\tilde{A}\tilde{z_i}\tilde{z_i}^\top + 2\tilde{A}(\tilde{x_i}(\tilde{z_i} - \tilde{y_i})^\top + (\tilde{z_i} - \tilde{y_i})\tilde{x_i}^\top) \\
&= 2\tilde{A}(\tilde{y_i}\tilde{y_i}^\top - \tilde{z_i}\tilde{z_i}^\top + \tilde{x_i}(\tilde{z_i} - \tilde{y_i})^\top + (\tilde{z_i} - \tilde{y_i})\tilde{x_i}^\top)
\end{align*}

\begin{equation}
\begin{aligned}
\nabla^2 g(\tilde{A})
&= \textrm{4d tensor}
\end{aligned}
\end{equation}

The Hessian is a 4d tensor, which complicates our analysis, so we will look at the case where $\tilde{A}$ is a vector as well. 

Let $f(w; x) = w^{\top}x$. Then we have:
\begin{equation}
\begin{aligned}
g(w) = (w^{\top}y)^2 - (w^{\top}z)^2 + 2(w^{\top}x)(w^{\top}(z - y))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})w
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla^2 g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})
\end{aligned}
\end{equation}

It is clear that we must impose some additional normalization of the data to ensure the Hessian is PSD, but it's not clear what that should be. One possibility is to use the eigenvalues of the data-dependent term as follows. Let $Q = \nabla^2 g(w) = 2(yy^{\top} - zz^{\top} + x(z - y)^{\top} + (z - y)x^{\top})$. 

\begin{equation}
\begin{aligned}
\nabla^2 \tilde{g}(w) = Q - diag(\lambda_{min}(Q))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\nabla \tilde{g}(w) = (Q - diag(\lambda_{min}(Q)))w
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\tilde{g}(w) = \frac{1}{2}w^{\top}(Q - diag(\lambda_{min}(Q)))w
\end{aligned}
\end{equation}

However, it is clear that this is trivial because the original problem is unbounded below if Q has a negative eigenvalue, and the new problem has a minimum of 0. The new problem doesn't do anything for us.

If it is possible to ensure that the problem in (\ref{eq:smooth_problem}) is convex and smooth, we can analyze it using the standard techniques, and hopefully find a bound which is better than the generic bound.

\section{DC Reformulation with Smoothing}

Let us return to the original max formulation to show that it is DC if we use L2-norm instead of L2-norm-squared. We also remove the norm constraint.

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, \| f(\theta; x_i) - f(\theta; y_i)\|_2 - \| f(\theta; x_i) - f(\theta; z_i)\|_2 + \alpha)
\end{aligned}
\end{equation}

Let $g_i(\theta) = \| f(\theta; x_i) - f(\theta; y_i)\|_2 + \alpha, \quad h_i(\theta) = f(\theta; x_i) - f(\theta; z_i)\|_2$. Note that $g_i(\theta)$ and $h_i(\theta)$ are convex if $f(\theta)$ is affine. We can rewrite the objective as:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(0, g_i(\theta) - h_i(\theta))
\end{aligned}
\end{equation}

Split up the max function:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(g_i(\theta), h_i(\theta)) - h_i(\theta)
\end{aligned}
\end{equation}

Finally, we can split up the sum to show the full DC formulation:

\begin{equation}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \max(g_i(\theta), h_i(\theta)) - \frac{1}{n} \sum_{i=1}^{n} h_i(\theta)
\end{aligned}
\end{equation}

Although this is DC for affine choice of $f(\theta)$, $h_i(\theta)$ is not smooth due to the max function and the L2-norm. To produce a smooth, convex approximation to the max function, we can use the LogSumExp function:

\begin{equation}
\begin{aligned}
\textrm{LSE}(x_1, ..., x_n) = \log\sum_{i=1}^{n} \exp(x_i)
\end{aligned}
\end{equation}

To produce a smooth approximation to the L2-norm, we can use the following:

\begin{equation}
\label{eq:smooth_2norm}
\begin{aligned}
\| x \|_\beta = \sqrt{\| x \|_2^2 + \beta^2} - \beta, \quad \beta > 0, x \in \mathbb{R}^d
\end{aligned}
\end{equation}

Note that Eq. (\ref{eq:smooth_2norm}) is $\frac{1}{\beta}$-smooth.

Using these functions, our final smoothed DC problem is:

\begin{equation}
\label{eq:smooth_dc}
\begin{aligned}
    \min_{\theta} \quad & \frac{1}{n} \sum_{i=1}^{n} \textrm{LSE}(g_i(\theta), h_i(\theta)) - \frac{1}{n} \sum_{i=1}^{n} h_i(\theta)
\end{aligned}
\end{equation}

The affine version is:

\begin{equation}
\begin{aligned}
    \min_{\tilde{A}} \quad & \frac{1}{n} \sum_{i=1}^{n} \log\left\{ \exp \left(\| \tilde{A}(\tilde{x}_i - \tilde{y}_i) \|_\beta + \alpha \right) + \exp \left(\| \tilde{A}(\tilde{x}_i - \tilde{z}_i) \|_\beta \right)  \right \} - \frac{1}{n} \sum_{i=1}^{n} \| \tilde{A}(\tilde{x}_i - \tilde{z}_i) \|_\beta
\end{aligned}
\end{equation}

To start, we examine a simpler case where $\tilde{A} = w \in \mathbb{R}^d$ and $n = 1$:

\begin{equation}
\begin{aligned}
    \min_{w} \quad & \log\left\{ \exp \left(\| w^{\top}(x - y) \|_\beta + \alpha \right) + \exp \left(\| w^{\top}(x - z) \|_\beta \right)  \right \} - \| w^{\top}(x - z) \|_\beta
\end{aligned}
\end{equation}

\subsection{Analysis of Gradient Method}

We analyze this formulation using the methodology from \cite{khamaru_convergence_nodate}. To use the theorem proven in that work, we need $g$ to be continuously differentiable and $M_g$-smooth, and we need $h$ to be continuous and convex. These properties are satisfied for affine choice of $f$ in Problem (\ref{eq:smooth_dc}).

Since $h_i(\theta)$ is differentiable for our case, we can use a regular gradient step and achieve the convergence bound for Algorithm 1 in \cite{khamaru_convergence_nodate}. Not sure if it is necessary to constrict $\theta \in \mathcal{C}$ as in \cite{khamaru_convergence_nodate}, but this can be done if necessary.

\subsubsection{Does this formulation have a trivial solution? Or a closed form solution?}

$A = w = 0$ does not minimize this function because of the margin parameter $\alpha$. As for finding a closed form solution, we can see if it is clear from computing the gradient.

\subsubsection{Find smoothness $M_g$ and smoothness $M_h$}

\subsubsection{Attempt improvement of convergence bound in \cite{khamaru_convergence_nodate} using the specific properties of our function}

\subsubsection{Does this smooth version allow for triplets which violate the original objective? If so, can this violation be quantified?}

\section{Potential Next Steps for the Project}

\begin{itemize}
  \item Trying different gradient methods
  \item Use Nesterov momentum as in the Princeton slides
  \item Analyzing stochastic gradient method for the problem
  \item Attempting to analyze the non-smoothed problem
  \item Empirical analysis using toy separable data with two classes
  \item Compare empirical behavior of gradient methods for the relaxed problem vs. the original problem
  \item Compare nearest neighbors performance for difference constraints on the weight space, C
\end{itemize}

\bibliography{references}

\end{document}
\grid
\grid