
@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: A unified embedding for face recognition and clustering},
	doi = {10.1109/CVPR.2015.7298682},
	shorttitle = {{FaceNet}},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called {FaceNet}, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with {FaceNet} embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild ({LFW}) dataset, our system achieves a new record accuracy of 99.63\%. On {YouTube} Faces {DB} it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	eventtitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {815--823},
	booktitle = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	date = {2015-06},
	note = {{ISSN}: 1063-6919},
	keywords = {neural nets, Training, Accuracy, Artificial neural networks, convolution, data mining, deep convolutional network, embedding optimization, Face, face clustering, face patch matching, face recognition, Face recognition, {FaceNet} embedding, image matching, online triplet mining method, optimisation, pattern clustering, Principal component analysis, Standards},
	file = {IEEE Xplore Abstract Record:/home/luke/Zotero/storage/M68X75CD/7298682.html:text/html;Submitted Version:/home/luke/Zotero/storage/QEARDG46/Schroff et al. - 2015 - FaceNet A unified embedding for face recognition .pdf:application/pdf}
}

@article{hermans_defense_2017,
	title = {In Defense of the Triplet Loss for Person Re-Identification},
	url = {https://arxiv.org/abs/1703.07737v4},
	abstract = {In the past few years, the field of computer vision has gone through a
revolution fueled mainly by the advent of large datasets and the adoption of
deep convolutional neural networks for end-to-end learning. The person
re-identification subfield is no exception to this. Unfortunately, a prevailing
belief in the community seems to be that the triplet loss is inferior to using
surrogate losses (classification, verification) followed by a separate metric
learning step. We show that, for models trained from scratch as well as
pretrained ones, using a variant of the triplet loss to perform end-to-end deep
metric learning outperforms most other published methods by a large margin.},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	urldate = {2020-04-09},
	date = {2017-03-22},
	langid = {english},
	file = {Full Text PDF:/home/luke/Zotero/storage/JVPNWENV/Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:application/pdf;Snapshot:/home/luke/Zotero/storage/224GD95U/1703.html:text/html}
}

@article{khamaru_convergence_nodate,
	title = {Convergence guarantees for a class of non-convex and non-smooth optimization problems},
	abstract = {We consider the problem of ﬁnding critical points of functions that are non-convex and nonsmooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simpliﬁcation of the popular {CCCP} algorithm, used for optimizing functions that can be written as a diﬀerence of two convex functions. Our simpliﬁed algorithm retains all the convergence properties of {CCCP}, along with a signiﬁcantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.},
	pages = {52},
	author = {Khamaru, Koulik and Wainwright, Martin J},
	langid = {english},
	file = {Khamaru and Wainwright - Convergence guarantees for a class of non-convex a.pdf:/home/luke/Zotero/storage/JKX6FESN/Khamaru and Wainwright - Convergence guarantees for a class of non-convex a.pdf:application/pdf}
}