
@article{weinberger_distance_2009,
	title = {Distance {Metric} {Learning} for {Large} {Margin} {Nearest} {Neighbor} {Classification}},
	volume = {10},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v10/weinberger09a.html},
	number = {Feb},
	urldate = {2020-03-27},
	journal = {Journal of Machine Learning Research},
	author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
	year = {2009},
	pages = {207--244},
	file = {Full Text PDF:/home/luke/Zotero/storage/2LZ4ZAH5/Weinberger and Saul - 2009 - Distance Metric Learning for Large Margin Nearest .pdf:application/pdf;Snapshot:/home/luke/Zotero/storage/CWKFADCL/weinberger09a.html:text/html}
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	shorttitle = {{FaceNet}},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {neural nets, Training, Accuracy, Artificial neural networks, convolution, data mining, deep convolutional network, embedding optimization, Face, face clustering, face patch matching, face recognition, Face recognition, FaceNet embedding, image matching, online triplet mining method, optimisation, pattern clustering, Principal component analysis, Standards},
	pages = {815--823},
	file = {IEEE Xplore Abstract Record:/home/luke/Zotero/storage/M68X75CD/7298682.html:text/html;Submitted Version:/home/luke/Zotero/storage/QEARDG46/Schroff et al. - 2015 - FaceNet A unified embedding for face recognition .pdf:application/pdf}
}

@article{hermans_defense_2017,
	title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
	url = {https://arxiv.org/abs/1703.07737v4},
	abstract = {In the past few years, the field of computer vision has gone through a
revolution fueled mainly by the advent of large datasets and the adoption of
deep convolutional neural networks for end-to-end learning. The person
re-identification subfield is no exception to this. Unfortunately, a prevailing
belief in the community seems to be that the triplet loss is inferior to using
surrogate losses (classification, verification) followed by a separate metric
learning step. We show that, for models trained from scratch as well as
pretrained ones, using a variant of the triplet loss to perform end-to-end deep
metric learning outperforms most other published methods by a large margin.},
	language = {en},
	urldate = {2020-04-09},
	author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	month = mar,
	year = {2017},
	file = {Full Text PDF:/home/luke/Zotero/storage/JVPNWENV/Hermans et al. - 2017 - In Defense of the Triplet Loss for Person Re-Ident.pdf:application/pdf;Snapshot:/home/luke/Zotero/storage/224GD95U/1703.html:text/html}
}

@article{khamaru_convergence_nodate,
	title = {Convergence guarantees for a class of non-convex and non-smooth optimization problems},
	abstract = {We consider the problem of ﬁnding critical points of functions that are non-convex and nonsmooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simpliﬁcation of the popular CCCP algorithm, used for optimizing functions that can be written as a diﬀerence of two convex functions. Our simpliﬁed algorithm retains all the convergence properties of CCCP, along with a signiﬁcantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.},
	language = {en},
	author = {Khamaru, Koulik and Wainwright, Martin J},
	pages = {52},
	file = {Khamaru and Wainwright - Convergence guarantees for a class of non-convex a.pdf:/home/luke/Zotero/storage/JKX6FESN/Khamaru and Wainwright - Convergence guarantees for a class of non-convex a.pdf:application/pdf}
}

@article{chen_smoothing_nodate,
	title = {Smoothing for nonsmooth optimization},
	language = {en},
	author = {Chen, Yuxin},
	pages = {30},
	file = {Chen - Smoothing for nonsmooth optimization.pdf:/home/luke/Zotero/storage/ZKI9IRAH/Chen - Smoothing for nonsmooth optimization.pdf:application/pdf}
}

@article{cook_basic_nodate,
	title = {Basic properties of the soft maximum},
	abstract = {This note presents the basic properties of the soft maximum, a smooth approximation to the maximum of two real variables. It concludes by looking at potential numerical diﬃculties with the soft maximum and how to avoid these diﬃculties.},
	language = {en},
	author = {Cook, John},
	pages = {4},
	file = {Cook - Basic properties of the soft maximum.pdf:/home/luke/Zotero/storage/8ET238HD/Cook - Basic properties of the soft maximum.pdf:application/pdf}
}

@inproceedings{qian_softtriple_2019,
	address = {Seoul, Korea (South)},
	title = {{SoftTriple} {Loss}: {Deep} {Metric} {Learning} {Without} {Triplet} {Sampling}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{SoftTriple} {Loss}},
	url = {https://ieeexplore.ieee.org/document/9008816/},
	doi = {10.1109/ICCV.2019.00655},
	abstract = {Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classiﬁcations, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classiﬁcation loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark ﬁne-grained data sets demonstrate the effectiveness of the proposed loss function.},
	language = {en},
	urldate = {2020-04-23},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Tacoma, Tacoma and Li, Hao and Jin, Rong},
	month = oct,
	year = {2019},
	pages = {6449--6457},
	file = {Qian et al. - 2019 - SoftTriple Loss Deep Metric Learning Without Trip.pdf:/home/luke/Zotero/storage/LPC36BMZ/Qian et al. - 2019 - SoftTriple Loss Deep Metric Learning Without Trip.pdf:application/pdf}
}

@article{amos_openface_nodate,
	title = {{OpenFace}: {A} general-purpose face recognition library with mobile applications},
	abstract = {Cameras are becoming ubiquitous in the Internet of Things (IoT) and can use face recognition technology to improve context. There is a large accuracy gap between today’s publicly available face recognition systems and the state-of-the-art private face recognition systems. This paper presents our OpenFace face recognition library that bridges this accuracy gap. We show that OpenFace provides near-human accuracy on the LFW benchmark and present a new classiﬁcation benchmark for mobile scenarios. This paper is intended for non-experts interested in using OpenFace and provides a light introduction to the deep neural network techniques we use.},
	language = {en},
	author = {Amos, Brandon and Ludwiczuk, Bartosz and Satyanarayanan, Mahadev},
	pages = {20},
	file = {Amos et al. - OpenFace A general-purpose face recognition libra.pdf:/home/luke/Zotero/storage/WI9VR5RK/Amos et al. - OpenFace A general-purpose face recognition libra.pdf:application/pdf}
}

@inproceedings{hadsell_dimensionality_2006,
	address = {New York, NY, USA},
	title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
	volume = {2},
	isbn = {978-0-7695-2597-6},
	url = {http://ieeexplore.ieee.org/document/1640964/},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that “similar” points in input space are mapped to nearby points on the manifold. Most existing techniques for solving the problem suffer from two drawbacks. First, most of them depend on a meaningful and computable distance metric in input space. Second, they do not compute a “function” that can accurately map new input samples whose relationship to the training data is unknown. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent non-linear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
	publisher = {IEEE},
	author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
	year = {2006},
	pages = {1735--1742},
	file = {Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf:/home/luke/Zotero/storage/NV59SNUD/Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf:application/pdf}
}

@article{sun_deep_nodate,
	title = {Deep {Learning} {Face} {Representation} by {Joint} {Identification}-{Verification}},
	abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identiﬁcation and veriﬁcation signals as supervision. The Deep IDentiﬁcation-veriﬁcation features (DeepID2) are learned with carefully designed deep convolutional networks. The face identiﬁcation task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face veriﬁcation task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15\% face veriﬁcation accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been signiﬁcantly reduced by 67\%.},
	language = {en},
	author = {Sun, Yi and Chen, Yuheng and Wang, Xiaogang and Tang, Xiaoou},
	pages = {9},
	file = {Sun et al. - Deep Learning Face Representation by Joint Identif.pdf:/home/luke/Zotero/storage/HIHF45VI/Sun et al. - Deep Learning Face Representation by Joint Identif.pdf:application/pdf}
}

@incollection{araujo_large_2009,
	address = {Berlin, Heidelberg},
	title = {Large {Scale} {Online} {Learning} of {Image} {Similarity} through {Ranking}},
	volume = {5524},
	isbn = {978-3-642-02171-8 978-3-642-02172-5},
	url = {http://link.springer.com/10.1007/978-3-642-02172-5_2},
	abstract = {Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or ﬁnding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions.},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
	editor = {Araujo, Helder and Mendonça, Ana Maria and Pinho, Armando J. and Torres, María Inés},
	year = {2009},
	doi = {10.1007/978-3-642-02172-5_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {11--14},
	file = {Chechik et al. - 2009 - Large Scale Online Learning of Image Similarity th.pdf:/home/luke/Zotero/storage/9W94GD2E/Chechik et al. - 2009 - Large Scale Online Learning of Image Similarity th.pdf:application/pdf}
}

@article{gao_properties_2018,
	title = {On the {Properties} of the {Softmax} {Function} with {Application} in {Game} {Theory} and {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.00805},
	abstract = {In this paper, we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature. In particular, we show that the softmax function is the monotone gradient map of the log-sum-exp function. By exploiting this connection, we show that the inverse temperature parameter λ determines the Lipschitz and co-coercivity properties of the softmax function. We then demonstrate the usefulness of these properties through an application in gametheoretic reinforcement learning.},
	language = {en},
	urldate = {2020-05-01},
	journal = {arXiv:1704.00805 [cs, math]},
	author = {Gao, Bolin and Pavel, Lacra},
	month = aug,
	year = {2018},
	note = {arXiv: 1704.00805},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Gao and Pavel - 2018 - On the Properties of the Softmax Function with App.pdf:/home/luke/Zotero/storage/TSY733VT/Gao and Pavel - 2018 - On the Properties of the Softmax Function with App.pdf:application/pdf}
}

@article{lipp_variations_2016,
	title = {Variations and extension of the convex–concave procedure},
	volume = {17},
	issn = {1389-4420, 1573-2924},
	url = {http://link.springer.com/10.1007/s11081-015-9294-x},
	doi = {10.1007/s11081-015-9294-x},
	abstract = {We investigate the convex–concave procedure, a local heuristic that utilizes the tools of convex optimization to ﬁnd local optima of difference of convex (DC) programming problems. The class of DC problems includes many difﬁcult problems such as the traveling salesman problem. We extend the standard procedure in two major ways and describe several variations. First, we allow for the algorithm to be initialized without a feasible point. Second, we generalize the algorithm to include vector inequalities. We then present several examples to demonstrate these algorithms.},
	language = {en},
	number = {2},
	urldate = {2020-05-03},
	journal = {Optimization and Engineering},
	author = {Lipp, Thomas and Boyd, Stephen},
	month = jun,
	year = {2016},
	pages = {263--287},
	file = {Lipp and Boyd - 2016 - Variations and extension of the convex–concave pro.pdf:/home/luke/Zotero/storage/ZD6H5E36/Lipp and Boyd - 2016 - Variations and extension of the convex–concave pro.pdf:application/pdf}
}

@book{calafiore_optimization_2014,
	title = {Optimization {Models}},
	isbn = {978-1-107-05087-7},
	abstract = {Emphasizing practical understanding over the technicalities of specific algorithms, this elegant textbook is an accessible introduction to the field of optimization, focusing on powerful and reliable convex optimization techniques. Students and practitioners will learn how to recognize, simplify, model and solve optimization problems - and apply these principles to their own projects. A clear and self-contained introduction to linear algebra demonstrates core mathematical concepts in a way that is easy to follow, and helps students to understand their practical relevance. Requiring only a basic understanding of geometry, calculus, probability and statistics, and striking a careful balance between accessibility and rigor, it enables students to quickly understand the material, without being overwhelmed by complex mathematics. Accompanied by numerous end-of-chapter problems, an online solutions manual for instructors, and relevant examples from diverse fields including engineering, data science, economics, finance, and management, this is the perfect introduction to optimization for undergraduate and graduate students.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Calafiore, Giuseppe C. and Ghaoui, Laurent El},
	month = oct,
	year = {2014},
	note = {Google-Books-ID: rEilBAAAQBAJ},
	keywords = {Business \& Economics / Operations Research, Business \& Economics / Statistics, Mathematics / Linear \& Nonlinear Programming, Mathematics / Probability \& Statistics / General, Technology \& Engineering / Electronics / General, Technology \& Engineering / Engineering (General), Technology \& Engineering / Mechanical, Technology \& Engineering / Telecommunications}
}

@article{lee_gradient_nodate,
	title = {Gradient {Descent} {Only} {Converges} to {Minimizers}},
	abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initialization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
	language = {en},
	author = {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
	pages = {12},
	file = {Lee et al. - Gradient Descent Only Converges to Minimizers.pdf:/home/luke/Zotero/storage/HLBL68HC/Lee et al. - Gradient Descent Only Converges to Minimizers.pdf:application/pdf}
}

@article{panageas_gradient_2016,
	title = {Gradient {Descent} {Only} {Converges} to {Minimizers}: {Non}-{Isolated} {Critical} {Points} and {Invariant} {Regions}},
	shorttitle = {Gradient {Descent} {Only} {Converges} to {Minimizers}},
	url = {http://arxiv.org/abs/1605.00405},
	abstract = {Given a non-convex twice differentiable cost function f, we prove that the set of initial conditions so that gradient descent converges to saddle points where {\textbackslash}nabla{\textasciicircum}2 f has at least one strictly negative eigenvalue has (Lebesgue) measure zero, even for cost functions f with non-isolated critical points, answering an open question in [Lee, Simchowitz, Jordan, Recht, COLT2016]. Moreover, this result extends to forward-invariant convex subspaces, allowing for weak (non-globally Lipschitz) smoothness assumptions. Finally, we produce an upper bound on the allowable step-size.},
	urldate = {2020-05-08},
	journal = {arXiv:1605.00405 [cs, math]},
	author = {Panageas, Ioannis and Piliouras, Georgios},
	month = jun,
	year = {2016},
	note = {arXiv: 1605.00405},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/home/luke/Zotero/storage/4CICIDHB/Panageas and Piliouras - 2016 - Gradient Descent Only Converges to Minimizers Non.pdf:application/pdf;arXiv.org Snapshot:/home/luke/Zotero/storage/4HF5GRHY/1605.html:text/html}
}

@inproceedings{liu_deep_2016,
	address = {Las Vegas, NV},
	title = {Deep {Relative} {Distance} {Learning}: {Tell} the {Difference} between {Similar} {Vehicles}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Deep {Relative} {Distance} {Learning}},
	url = {https://ieeexplore.ieee.org/document/7780607/},
	doi = {10.1109/CVPR.2016.238},
	abstract = {The growing explosion in the use of surveillance cameras in public security highlights the importance of vehicle search from a large-scale image or video database. However, compared with person re-identiﬁcation or face recognition, vehicle search problem has long been neglected by researchers in vision community. This paper focuses on an interesting but challenging problem, vehicle re-identiﬁcation (a.k.a precise vehicle search). We propose a Deep Relative Distance Learning (DRDL) method which exploits a two-branch deep convolutional network to project raw vehicle images into an Euclidean space where distance can be directly used to measure the similarity of arbitrary two vehicles. To further facilitate the future research on this problem, we also present a carefully-organized largescale image database “VehicleID”, which includes multiple images of the same vehicle captured by different realworld cameras in a city. We evaluate our DRDL method on our VehicleID dataset and another recently-released vehicle model classiﬁcation dataset “CompCars” in three sets of experiments: vehicle re-identiﬁcation, vehicle model veriﬁcation and vehicle retrieval. Experimental results show that our method can achieve promising results and outperforms several state-of-the-art approaches.},
	language = {en},
	urldate = {2020-05-08},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Hongye and Tian, Yonghong and Wang, Yaowei and Pang, Lu and Huang, Tiejun},
	month = jun,
	year = {2016},
	pages = {2167--2175},
	file = {Liu et al. - 2016 - Deep Relative Distance Learning Tell the Differen.pdf:/home/luke/Zotero/storage/UMVIPME9/Liu et al. - 2016 - Deep Relative Distance Learning Tell the Differen.pdf:application/pdf}
}

@misc{noauthor_about_nodate,
	title = {About {Face} {ID} advanced technology},
	url = {https://support.apple.com/en-us/HT208108},
	abstract = {Learn how Face ID helps protect your information on your iPhone and iPad Pro.},
	language = {en},
	urldate = {2020-05-08},
	journal = {Apple Support},
	note = {Library Catalog: support.apple.com},
	file = {Snapshot:/home/luke/Zotero/storage/E792R9UY/HT208108.html:text/html}
}

@article{mccartney_are_2019,
	chapter = {Life},
	title = {Are {You} {Ready} for {Facial} {Recognition} at the {Airport}?},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/are-you-ready-for-facial-recognition-at-the-airport-11565775008},
	abstract = {Airlines and TSA are starting to scan faces to get people through security and boarding faster, leading privacy advocates to warn of unintended consequences.},
	language = {en-US},
	urldate = {2020-05-08},
	journal = {Wall Street Journal},
	author = {McCartney, Scott},
	month = aug,
	year = {2019},
	keywords = {air transport, airlines, airport, airports, boarding pass, business, cbp, consumer services, DAL, Delta Air Lines, drivers license, facial id, facial recognition, general news, information security, leasing services, lifestyle, living, logistics, national, passport, political, privacy, privacy issues, public security, rental, security, transportation, travel, tsa},
	file = {Snapshot:/home/luke/Zotero/storage/BDVFJE63/are-you-ready-for-facial-recognition-at-the-airport-11565775008.html:text/html}
}

@misc{noauthor_technology_nodate,
	title = {Technology - {TinEye}},
	url = {https://tineye.com/technology},
	urldate = {2020-05-08},
	file = {Technology - TinEye:/home/luke/Zotero/storage/ZQWPBYPI/technology.html:text/html}
}

@misc{noauthor_find_nodate,
	title = {Find related images with reverse image search - {Computer} - {Google} {Search} {Help}},
	url = {https://support.google.com/websearch/answer/1325808?co=GENIE.Platform%3DDesktop&hl=en},
	urldate = {2020-05-08},
	file = {Find related images with reverse image search - Computer - Google Search Help:/home/luke/Zotero/storage/DEV69E7P/1325808.html:text/html}
}

@inproceedings{gaillard_large_2017,
	title = {Large scale reverse image search - {A} method comparison for almost identical image retrieval},
	abstract = {In this paper, we presented our study and benchmark on Reverse Image Search (RIS) methods, with a special focus on finding almost similar images in a very large image collection. In our framework we concentrate our study on radius (threshold) based image search methods. We focused our study on perceptual hash based solutions for their scalability, but other solutions seem to give also good results. We studied the speed and the accuracy (precision/recall) of several existing image features. We also proposed a two-layer method that combines a fast but not very precise method with a slower but more accurate method to provide a scalable and precise RIS system. MOTS-CLES : recherche d’images inversé, pHash, optimisation, SI images},
	booktitle = {{INFORSID}},
	author = {Gaillard, Mathieu and Egyed-Zsigmond, Elöd},
	year = {2017}
}